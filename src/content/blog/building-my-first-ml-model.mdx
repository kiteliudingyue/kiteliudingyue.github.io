---
title: "Building My First Machine Learning Model: Lessons Learned"
description: "A reflection on developing a customer segmentation model, including the mistakes I made and the insights I gained along the way."
pubDate: 2025-11-15
tags: ["Machine Learning", "Python", "Data Science", "Personal"]
---

Last month, I shipped my first production machine learning model—a customer segmentation system that now powers marketing decisions for a 100K+ user e-commerce platform. Here's what I learned (and wish I knew earlier).

## The Problem

The marketing team was sending the same campaigns to everyone. Conversion rates were stuck at 2.3%, and they needed a way to target different customer groups with personalized messaging.

**Goal**: Segment customers based on behavior to enable targeted campaigns.

## Initial Approach (That Failed)

My first instinct was to jump straight into neural networks. I mean, deep learning is cool, right?

```python
# What I tried first (spoiler: overkill)
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(5, activation='softmax')
])
```

**Why it failed:**
- Required tons of labeled data (which we didn't have)
- Took forever to train
- Results weren't interpretable
- Team couldn't understand or trust it

## What Actually Worked

I stepped back and chose **K-means clustering**—a simpler, unsupervised approach:

```python
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd

# 1. Prepare features: RFM (Recency, Frequency, Monetary)
def calculate_rfm(df):
    current_date = df['purchase_date'].max()

    rfm = df.groupby('customer_id').agg({
        'purchase_date': lambda x: (current_date - x.max()).days,  # Recency
        'order_id': 'count',  # Frequency
        'total_amount': 'sum'  # Monetary
    })

    rfm.columns = ['recency', 'frequency', 'monetary']
    return rfm

# 2. Standardize features
scaler = StandardScaler()
rfm_scaled = scaler.fit_transform(rfm)

# 3. Find optimal clusters using elbow method
inertias = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(rfm_scaled)
    inertias.append(kmeans.inertia_)

# 4. Train final model
optimal_k = 5  # Determined from elbow plot
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
clusters = kmeans.fit_predict(rfm_scaled)
```

## Key Lessons

### 1. Start Simple

Complex models aren't always better. K-means was:
- Easy to explain to stakeholders
- Fast to train (< 1 minute)
- Interpretable results
- Good enough for the use case

**Rule of thumb**: Try the simplest model that could work first. Add complexity only if needed.

### 2. Feature Engineering Matters More Than Model Choice

I spent weeks tweaking hyperparameters for a 1% improvement. Then I added two new features (time since last purchase, average order value) and got a 12% improvement instantly.

**Insight**: Better features beat better algorithms.

### 3. Domain Knowledge is Critical

I initially created 8 segments. The marketing team said "that's too many—we can only handle 4-5 campaigns."

**Lesson**: Talk to stakeholders early and often. Your technical solution needs to fit their operational constraints.

### 4. Validate with Business Metrics, Not Just ML Metrics

My silhouette score was great (0.52), but that didn't matter to the business. What mattered:

- Did campaigns improve conversion rates? **Yes: 2.3% → 6.4%**
- Could the team action on the segments? **Yes**
- Were segments stable over time? **Yes: 87% stability month-over-month**

## The Final Segmentation

We identified 5 distinct customer segments:

| Segment | % of Customers | % of Revenue | Strategy |
|---------|----------------|--------------|----------|
| **Champions** | 8% | 42% | Reward & retain |
| **Loyal** | 22% | 31% | Upsell premium products |
| **Potential** | 18% | 15% | Nurture with engagement campaigns |
| **At-Risk** | 31% | 9% | Win-back campaigns |
| **Lost** | 21% | 3% | Reactivation offers |

## Implementation Challenges

### Challenge 1: Batch vs. Real-Time

Initially, I ran clustering weekly in batch. But marketing wanted near real-time updates.

**Solution**: Pre-compute cluster centroids, then assign new customers to nearest cluster in real-time.

```python
# Save trained model
import joblib
joblib.dump(kmeans, 'customer_segments_model.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')

# Real-time prediction API
def predict_segment(customer_data):
    features = calculate_rfm(customer_data)
    features_scaled = scaler.transform(features)
    segment = kmeans.predict(features_scaled)[0]
    return segment
```

### Challenge 2: Model Drift

Customer behavior changes over time. Segments that made sense in January were less relevant by June.

**Solution**:
- Monitor segment distributions monthly
- Retrain quarterly
- Set up alerts for sudden distribution shifts

```python
# Monitor segment distribution
current_dist = df['segment'].value_counts(normalize=True)
baseline_dist = pd.read_csv('baseline_distribution.csv')

# Alert if any segment changes by >15%
drift = abs(current_dist - baseline_dist['proportion'])
if (drift > 0.15).any():
    send_alert("Segment drift detected!")
```

## Results After 3 Months

- **Conversion rate**: 2.3% → 6.4% (178% improvement)
- **Marketing ROI**: Up 3.2x
- **Customer satisfaction**: NPS improved from 42 to 56
- **Team adoption**: 100% of campaigns now use segmentation

## What I'd Do Differently

1. **Start with exploratory data analysis**: I jumped into modeling too fast. Spend more time understanding your data first.

2. **Build interpretability tools**: I should have created better visualizations to explain segments to stakeholders from day one.

3. **Plan for production earlier**: I treated this as an analysis project, not a product. Infrastructure setup was an afterthought.

4. **Document everything**: Future me (and my teammates) would appreciate better documentation of feature engineering decisions.

## Resources That Helped

- [scikit-learn clustering docs](https://scikit-learn.org/stable/modules/clustering.html)
- "Hands-On Machine Learning" by Aurélien Géron (Chapter 9)
- [RFM Analysis explained](https://clevertap.com/blog/rfm-analysis/)

## Final Thoughts

My biggest takeaway: **Ship something simple that works, then iterate.** I wasted weeks on complex models that never saw production. The simple K-means model I shipped in 2 days created real business value.

Machine learning is powerful, but remember—it's a tool to solve business problems, not an end in itself.

What was your first ML project? What lessons did you learn? I'd love to hear your stories.

---

*This post is part of my "Lessons Learned" series where I share practical insights from real projects. Follow along for more data science and engineering content.*
